# ood-detection

This project examines the problem of out-of-distribution (OOD) detection in machine learning, which is the situation where a network is exposed to data from classes outside of the distribution of classes seen during training. For example, a network trained to identify dog breeds will still attempt to predict the dog breed pictured in an image of a cat and output a confident prediction. In reality, this overly confident prediction should not be output, so being able to detect an OOD input and refuse to make this error is valuable for a model.  

A variety of methods for OOD detection were examined from the literature, including the baseline method, ODIN, and the energy method. Outlier exposure was also implemented during training, where OOD samples are included while training a model. All experiments were conducted using residual networks (ResNets) trained on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). The methods themselves were compared to each other, as well as their performance when evaluated on test images from a variety of OOD datasets. We also experimented with changing the depth of the model. All methods and results are detailed in the poster and report hosted here.
